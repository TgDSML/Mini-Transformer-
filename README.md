# ðŸ§  Mini-Transformer (From Scratch)

This repository contains a **minimal yet complete implementation of a Transformer encoder built from scratch in PyTorch**.  
The project is designed as a **learning-oriented, research-style implementation**, focusing on understanding and validating the **core building blocks of modern Transformer architectures** rather than relying on high-level libraries.

The implementation closely follows the original paper:  
**â€œAttention Is All You Needâ€ (Vaswani et al., 2017)**

---

## ðŸ“Œ Project Objectives

- Implement a **Transformer encoder from scratch** using PyTorch
- Gain a **deep, practical understanding** of:
  - Self-attention
  - Multi-head attention
  - Positional encodings
  - Feed-forward networks
  - Residual connections & layer normalization
- Build a **clean, modular, and testable codebase**
- Validate correctness using **unit tests**
- Serve as a **reference project** for understanding Transformer internals

---

## ðŸ§© Core Components Implemented

- Scaled Dot-Product Attention
- Multi-Head Attention
- Positional Encoding
  - Sinusoidal
  - Learnable (optional)
- Position-wise Feed-Forward Network
- Transformer Encoder Layer
- Residual connections & Layer Normalization
- End-to-end forward pass
- Gradient-safe architecture (verified via tests)

---

## ðŸ“‚ Repository Structure

Mini-Transformer-/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ (optional datasets or toy data)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ attention.py
â”‚   â”‚   â”œâ”€â”€ feedforward.py
â”‚   â”‚   â”œâ”€â”€ positional_encoding.py
â”‚   â”‚   â””â”€â”€ normalization.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ encoder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ helper functions
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_attention.py
â”‚   â”œâ”€â”€ test_feedforward.py
â”‚   â”œâ”€â”€ test_positional_encoding.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

---

## ðŸ§ª Testing Philosophy

This project places strong emphasis on **correctness and reliability**.

Unit tests verify:

- Shape consistency
- Proper attention behavior
- Residual connections preserving dimensions
- Positional encoding correctness
- Gradient flow through the encoder

Run all tests with:

    pytest -q

---

## âš™ï¸ Installation & Setup

Clone repository:

    git clone https://github.com/TgDSML/Mini-Transformer-.git
    cd Mini-Transformer-

Create virtual environment:

    python -m venv .venv

Activate environment:

Windows:
    .\.venv\Scripts\Activate.ps1

macOS / Linux:
    source .venv/bin/activate

Install dependencies:

    pip install -r requirements.txt

---

## ðŸš€ How to Run

Run the main script:

    python main.py

The `main.py` file performs a **forward pass through the Transformer encoder**, typically using randomly generated input or a small toy example, to validate:

- Model construction
- Forward propagation
- Output shapes
- Gradient flow

> This project is intentionally minimal and does **not** include full training on large datasets.

---

## ðŸ§  Design Philosophy

- **Clarity over abstraction**
- **Explicit implementations** instead of magic wrappers
- **Educational value first**
- Modular components that mirror the Transformer paper structure
- Suitable for:
  - Learning
  - Teaching
  - Interview preparation
  - Further research extensions

---

## ðŸ“ˆ Project Status

- âœ… Scaled Dot-Product Attention implemented
- âœ… Multi-Head Attention implemented
- âœ… Positional Encodings (sinusoidal & learnable)
- âœ… Transformer Encoder Layer assembled
- âœ… Comprehensive unit tests
- ðŸš§ Extensions & experimentation ongoing

---

## ðŸ”® Future Improvements

- Decoder implementation
- Full Transformer (Encoder-Decoder)
- Training loop on a toy language modeling task
- Attention visualization
- Benchmark against PyTorchâ€™s `nn.Transformer`

---

## ðŸ“š References

- Vaswani et al., *Attention Is All You Need*, 2017
- PyTorch documentation

---

## ðŸ“Œ Notes

This repository is part of a **hands-on learning journey into Transformers and modern NLP architectures**, and is intentionally kept lightweight and readable.


