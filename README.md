# ğŸ§  Mini-Transformer (From Scratch)

This repository contains a **minimal yet complete implementation of a Transformer encoder built from scratch in PyTorch**.  
The project is designed as a **learning-oriented, research-style implementation**, focusing on understanding and validating the **core building blocks of modern Transformer architectures** rather than relying on high-level libraries.

The implementation closely follows the original paper:  
**â€œAttention Is All You Needâ€ (Vaswani et al., 2017)**

---

## ğŸ“Œ Project Objectives

- Implement a **Transformer encoder from scratch** using PyTorch
- Gain a **deep, practical understanding** of:
  - Self-attention
  - Multi-head attention
  - Positional encodings
  - Feed-forward networks
  - Residual connections & layer normalization
- Build a **clean, modular, and testable codebase**
- Validate correctness using **unit tests**
- Serve as a **reference project** for understanding Transformer internals

---

## ğŸ§© Core Components Implemented

- Scaled Dot-Product Attention  
- Multi-Head Attention  
- Positional Encoding  
  - Sinusoidal  
  - Learnable (optional)  
- Position-wise Feed-Forward Network  
- Transformer Encoder Layer  
- Residual connections & Layer Normalization  
- End-to-end forward pass  
- Gradient-safe architecture (verified via tests)

---

## ğŸ“‚ Repository Structure


