# Mini-Transformer (from scratch)

A minimal **Transformer** implementation built from scratch in **PyTorch**, trained and evaluated for **next-token prediction**.  
This project focuses on clarity, correctness, and learning: implementing core Transformer blocks and validating them with tests.

> Repo: TgDSML/Mini-Transformer-


## What’s inside

- Transformer building blocks (attention, FFN, positional encoding, residuals, layer norm)
- Training & evaluation workflow for next-token prediction
- Unit tests to validate core components

## Project structure

Mini-Transformer-/
├── data/ # datasets / prepared data (if any)
├── src/ # source code (model, layers, utils, etc.)
├── tests/ # unit tests
├── main.py # entry point (train/eval)
└── requirements.txt # dependencies
